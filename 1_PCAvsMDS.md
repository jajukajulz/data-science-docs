Title:       PCA vs MDS  
Author:      Julian Kanjere  
Date:        October 2019

| Criteria        | PCA           | MDS  | Comment  |
| ------------- |:-------------:| -----:| -----:|
| Definition      | Multivariate Dimension Reduction technique used to find the core components of a dataset with the help of orthogonal transformations. This linear transformation reveals the internal structure of the dataset with an arbitrarily designed new basis in the vector space, which best explains the variance of the data. | Multivariate dimension reduction technique that is used to visualize the similarity/dissimilarity between samples by plotting points in a lower dimensional splace. | Both are dimension reduction techniques.|
| Distance Measure      | Uses Euclidean distance.      |   Can use other distance measures, not strictly Euclidean e.g. Manhattan. |   Classic MDS with Euclidean distance same as Principal Component Analysis with 2 Principal Componentsâ€™s. |
| Data Linearity 	 | Works best with linear data.     |  Can work with linear or non-linear data. |   PCA tries to find hidden linear correlation among the variables. Hence, if the variables have non-linear relation, then the technique does not perform well. In contrast, MDS tries to preserve the topology of the data, and it is inherently a non-linear transformation. |
| Output from Dimension Reduction | Returns the principal components that are equal to the number of variables in the original dataset that was fed into the Principal Component Analysis algorithm.     | Returns the number of dimensions specified by data analyst. |   - |
| Input Data | Data matrix.     |    Dissimilarity matrix, representing the distances between pairs of objects. | - |
| Preservation | This Dimensionality Reduction technique seeks to preserve the variance from the original dataset.    | This Dimensionality Reduction technique seeks to preserve the distance between observations from the original dataset. |  - |
| Types | PCA generally one type which is performed using Eigen decomposition or Singular value decomposition.      |  Metric (Classical) and Non-metric (Ordinal). Metric preserves the original distance metric between points. Non-metric constructs fitted distances that are in the same rank order as the original distance. Metric is suitable for quantitative data and Non-metric is suitable for qualitative data. |    - |
| Objective | Focused on the dimensions themselves, and seek to maximize explained variance. | Focused on relations among the scaled objects. |    - |
| Data Projection | Projects a multidimensional space to the directions of maximum variability using covariance/correlation matrix to analyze the correlation between data points and variables.|Projects n-dimensional data points to a lower dimensional space (often 2-dimensional space) such that similar objects in the n-dimensional space will be close together on the lower dimensional plot. |- |
| R Function | prcomp() from STATS package which uses singular value decomposition of the input data matrix, princomp() from STATS package which uses eigen decomposition of the input data matrix.  | Non-metric MDS - isoMDS() from MASS package, Metric MDS - cmdscale() from STATS package. |   -|
| Outlier Resistance | Performs well if the objects in the data set do not contain oddities like outliers (which introduce a leverage effect disturbing the quality of the projection).     |    Since MDS tries to preserve the topology of the data, and it is inherently a non-linear transformation, it is not negatively affected by outliers. |    Both PCA and MDS can be used for outlier detection. |